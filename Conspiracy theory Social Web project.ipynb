{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Social Web - Conspiracy theories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explaining the project..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install and import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: textblob in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (0.15.3)\n",
      "Requirement already satisfied, skipping upgrade: nltk>=3.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from textblob) (3.5)\n",
      "Requirement already satisfied, skipping upgrade: click in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from nltk>=3.1->textblob) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: joblib in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from nltk>=3.1->textblob) (0.17.0)\n",
      "Requirement already satisfied, skipping upgrade: regex in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from nltk>=3.1->textblob) (2020.11.13)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from nltk>=3.1->textblob) (4.53.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 20.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.8/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -U textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Yossigijrath/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "import tweepy as tw\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import networkx as nx\n",
    "from textblob import TextBlob\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Twitter setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = 'N4J4jiQuGW7DZKLvUXg1DUYlw'\n",
    "consumer_secret = 'HwhmSr8rGqbzG9NlfaOSHl4No2RijgtJwtDCQEFNZ5vlSPzYbj'\n",
    "access_token = '158063657-YJjxwBnlFS6GGn0jkhnqA6PIjzVRC9UhACSjCWrX'\n",
    "access_token_secret = 'kuwpm8QnoNIX3bTkgOrlhjbdEKYxRiFqxqRXXRdLGeoJF'\n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the search term and the date_since date as variables\n",
    "hashtag_word = \"#electionfraud\"\n",
    "search_words = \"#electionfraud -filter:retweets\"\n",
    "collection_words = ['electionfraud']\n",
    "date_since = \"2020-07-01\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Twitter hashtag scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect tweets\n",
    "tweets = tw.Cursor(api.search,\n",
    "                   q=search_words,\n",
    "                   lang=\"en\",\n",
    "                   since=date_since).items(1000)\n",
    "\n",
    "all_tweets = [tweet.text for tweet in tweets]\n",
    "\n",
    "all_tweets[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pre-prossesing and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(txt):\n",
    "    \"\"\"Replace URLs found in a text string with nothing \n",
    "    (i.e. it will remove the URL from the string).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    txt : string\n",
    "        A text string that you want to parse and remove urls.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The same txt string with url's removed.\n",
    "    \"\"\"\n",
    "\n",
    "    return \" \".join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", txt).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove URLs\n",
    "all_tweets_no_urls = [remove_url(tweet) for tweet in all_tweets]\n",
    "\n",
    "# Create a sublist of lower case words for each tweet\n",
    "words_in_tweet = [tweet.lower().split() for tweet in all_tweets_no_urls]\n",
    "\n",
    "# Adding stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stop words from each tweet list of words\n",
    "tweets_nsw = [[word for word in tweet_words if not word in stop_words]\n",
    "              for tweet_words in words_in_tweet]\n",
    "\n",
    "# Remove collection words\n",
    "tweets_nsw_nc = [[w for w in word if not w in collection_words]\n",
    "                 for word in tweets_nsw]\n",
    "\n",
    "all_words_nsw_nc = list(itertools.chain(*tweets_nsw_nc))\n",
    "\n",
    "# Create counter of words in clean tweets\n",
    "counts_nsw_nc = collections.Counter(all_words_nsw_nc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Graph of most used words for given hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweets_ncw = pd.DataFrame(counts_nsw_nc.most_common(20),\n",
    "                             columns=['words', 'count'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Plot horizontal bar graph\n",
    "clean_tweets_ncw.sort_values(by='count').plot.barh(x='words',\n",
    "                      y='count',\n",
    "                      ax=ax,\n",
    "                      color=\"red\")\n",
    "\n",
    "ax.set_title(\"Common Words Found in Tweets on %s\" % hashtag_word)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Word bigrams and Network analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_bigram = [list(bigrams(tweet)) for tweet in tweets_nsw_nc]\n",
    "\n",
    "# Flatten list of bigrams in clean tweets\n",
    "bigrams = list(itertools.chain(*terms_bigram))\n",
    "\n",
    "# Create counter of words in clean bigrams\n",
    "bigram_counts = collections.Counter(bigrams)\n",
    "\n",
    "bigram_counts.most_common(20)\n",
    "bigram_df = pd.DataFrame(bigram_counts.most_common(20),\n",
    "                             columns=['bigram', 'count'])\n",
    "\n",
    "bigram_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of bigrams and their counts\n",
    "d = bigram_df.set_index('bigram').T.to_dict('records')\n",
    "\n",
    "# Create network plot \n",
    "G = nx.Graph()\n",
    "\n",
    "# Create connections between nodes\n",
    "for k, v in d[0].items():\n",
    "    G.add_edge(k[0], k[1], weight=(v * 10))\n",
    "\n",
    "G.add_node(\"conspiracy\", weight=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "pos = nx.spring_layout(G, k=2)\n",
    "\n",
    "# Plot networks\n",
    "nx.draw_networkx(G, pos,\n",
    "                 font_size=16,\n",
    "                 width=3,\n",
    "                 edge_color='grey',\n",
    "                 node_color='red',\n",
    "                 with_labels = False,\n",
    "                 ax=ax)\n",
    "\n",
    "# Create offset labels\n",
    "for key, value in pos.items():\n",
    "    x, y = value[0]+.135, value[1]+.045\n",
    "    ax.text(x, y,\n",
    "            s=key,\n",
    "            bbox=dict(facecolor='blue', alpha=0.25),\n",
    "            horizontalalignment='center', fontsize=13)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create textblob objects of the tweets\n",
    "sentiment_objects = [TextBlob(tweet) for tweet in all_tweets_no_urls]\n",
    "\n",
    "# Create list of polarity valuesx and tweet text\n",
    "sentiment_values = [[tweet.sentiment.polarity, str(tweet)] for tweet in sentiment_objects]\n",
    "\n",
    "# Create dataframe containing the polarity value and tweet text\n",
    "sentiment_df = pd.DataFrame(sentiment_values, columns=[\"polarity\", \"tweet\"])\n",
    "\n",
    "# Remove polarity values equal to zero\n",
    "sentiment_df = sentiment_df[sentiment_df.polarity != 0]\n",
    "\n",
    "sentiment_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot histogram with break at zero\n",
    "sentiment_df.hist(bins=[-1, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1],\n",
    "             ax=ax,\n",
    "             color=\"purple\")\n",
    "\n",
    "plt.title(\"Sentiments from Tweets on %s\" % hashtag_word)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
